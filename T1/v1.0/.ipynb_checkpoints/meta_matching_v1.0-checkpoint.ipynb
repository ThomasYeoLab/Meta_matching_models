{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Meta matching v1.0\n",
    "This jupyter notebook demonstrates you how to load and use meta-matching algorithm. In this demonstration, we performed meta-matching with 20 example subjects.\n",
    "\n",
    "Package needed (and version this jupyter notebook tested):\n",
    "* Numpy (1.19.2)\n",
    "* Scipy (1.5.2)\n",
    "* PyTorch (1.7.1)\n",
    "* Scikit-learn (0.23.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Setup\n",
    "Please modify the `path_repo` below to your repo position:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_repo = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialization and random seed set\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import scipy\n",
    "import torch\n",
    "import pickle\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. load data\n",
    "Load the example fake data that we provided, it contains\n",
    "* Example input structural MRI T1 `x` with size of (20, 182x218x182)\n",
    "    * 20 is number of subjects\n",
    "    * 182x218x182 is dimension of 3D T1 data\n",
    "* Example output phenotypes `y` with size of (20, 2)\n",
    "    * 2 is number of phenotypes.\n",
    "* Example icv data `icv` with size of (20, 1)\n",
    "    * 1 is dimension of icv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 182, 218, 182) (20, 2) (20, 1)\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(path_repo, 'data')\n",
    "model_path = os.path.join(path_repo, 'model')\n",
    "\n",
    "from CBIG_model_pytorch import znorm_icv\n",
    "\n",
    "npz = np.load(os.path.join(data_path, 'meta_matching_v1.0_data.npz'))\n",
    "x_input = npz['x']\n",
    "y_input = npz['y']\n",
    "icv_input = npz['icv']\n",
    "icv_input = znorm_icv(icv_input)\n",
    "print(x_input.shape, y_input.shape, icv_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Split data\n",
    "Here, we also split 20 subjects to 80/20, where 80 for training, and 20 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 182, 218, 182) (4, 182, 218, 182) (16, 1) (4, 1) (16, 2) (4, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from CBIG_model_pytorch import mics_z_norm\n",
    "\n",
    "x_train, x_test, icv_train, icv_test, y_train, y_test = train_test_split(x_input, icv_input, y_input, test_size=0.2, random_state=42)\n",
    "n_subj_train, n_subj_test = x_train.shape[0], x_test.shape[0]\n",
    "y_train, y_test, _, _ = mics_z_norm(y_train, y_test)\n",
    "print(x_train.shape, x_test.shape, icv_train.shape, icv_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Meta-matching models predict\n",
    "Here we apply the model pretrained on large source dataset (UK Biobank) to predict source phenotypes on `x_train` and `x_test`. We will get the predicted 67 source phenotypes on both 16 training subjects and 4 test subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/CBIG_ukbb_dnn_run_0_epoch_98.pkl_torch\n",
      "./model/CBIG_ukbb_dnn_run_0_epoch_98.pkl_torch\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:145] . PytorchStreamReader failed reading zip archive: invalid header or archive is corrupted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(weight_path)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCBIG_model_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metamatching_infer\n\u001b[0;32m----> 9\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmetamatching_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43micv_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m metamatching_infer(x_test, icv_test, y_test, model_path)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train_pred\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, y_train_pred)\n",
      "File \u001b[0;32m/mnt/isilon/CSC1/Yeolab/Users/nwulan/Meta_matching_models/T1/v1.0/CBIG_model_pytorch.py:254\u001b[0m, in \u001b[0;36mmetamatching_infer\u001b[0;34m(x, icv, y, model_dir)\u001b[0m\n\u001b[1;32m    249\u001b[0m weight_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    250\u001b[0m     model_dir,\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCBIG_ukbb_dnn_run_0_epoch_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(opt_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl_torch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mprint\u001b[39m(weight_path)\n\u001b[0;32m--> 254\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# map_location=torch.device('cpu')\u001b[39;00m\n\u001b[1;32m    255\u001b[0m net\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    256\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/isilon/CSC1/Yeolab/Users/nwulan/anaconda3/envs/Naren2024_MMT1/lib/python3.8/site-packages/torch/serialization.py:587\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m--> 587\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    589\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    590\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    591\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/isilon/CSC1/Yeolab/Users/nwulan/anaconda3/envs/Naren2024_MMT1/lib/python3.8/site-packages/torch/serialization.py:242\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_zipfile_reader, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:145] . PytorchStreamReader failed reading zip archive: invalid header or archive is corrupted"
     ]
    }
   ],
   "source": [
    "from CBIG_model_pytorch import metamatching_infer\n",
    "\n",
    "y_train_pred = metamatching_infer(x_train, icv_train, y_train, model_path)\n",
    "y_test_pred = metamatching_infer(x_test, icv_test, y_test, model_path)\n",
    "\n",
    "print(y_train_pred.shape, '\\n', y_train_pred)\n",
    "print(y_test_pred.shape, '\\n', y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Stacking\n",
    "Perform stacking with `y_train_pred`, `y_test_pred`, `y_train`, where we use the prediction of 16 subjects `y_train_pred` (input) and real data `y_train` (output) to train the stacking model, then we applied the model to `y_test_pred` to get final prediction of 2 phenotypes on 4 subjects. Here\n",
    "for simplicity of the example code, we use all 67 outputs from pretrained model as the input of stacking KRR model, if you want to select the top K outputs please see our [CBIG repo](https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/predict_phenotypes/Naren2024_MMT1) for more details.\n",
    "\n",
    "#### Hyperparameter Tuning\n",
    "In `stacking()` function, we set the range of `alpha` as `[5, 10, 15, 20, 25, 30, 35, 40, 45, 50]`. You are weclomed to modify the range of `alpha` to get better performance on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from CBIG_model_pytorch import stacking\n",
    "\n",
    "y_test_final_arr = np.zeros((y_test_pred.shape[0], y_train.shape[1]))\n",
    "y_train_final_arr = np.zeros((y_train_pred.shape[0], y_train.shape[1]))\n",
    "for i in range(y_train.shape[1]):\n",
    "    # For each test phenotype, perform stacking by developing a KRR model\n",
    "    y_test_final, y_train_final = stacking(y_train_pred, y_test_pred, y_train[:,i])\n",
    "    y_test_final_arr[:,i] = y_test_final\n",
    "    y_train_final_arr[:,i] = y_train_final\n",
    "print(y_test_final_arr.shape, '\\n', y_test_final_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Evaluation\n",
    "Evaluate the prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "corr = np.zeros((y_train.shape[1]))\n",
    "for i in range(y_train.shape[1]):\n",
    "    corr[i] = pearsonr(y_test_final_arr[:, i], y_test[:, i])[0]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Haufe transform predictive network features (PNFs) computation\n",
    "Here we compute the PNF for stacking we just performed. It computes the covariance between 2 phenotype predicitons and each voxel of 3D T1 data on the 16 training subjects. The final PNF is in shape of (87571, 2), where 87571 is number of voxel after crop, and 2 is number of phenotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from CBIG_model_pytorch import covariance_rowwise, load_3D_input\n",
    "\n",
    "x_train = load_3D_input(x_train)\n",
    "cov = covariance_rowwise(x_train, y_train_final_arr)\n",
    "print(cov, '\\n', cov.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
