{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta matching v2.0\n",
    "This jupyter notebook demonstrates you how to load and use multilayer meta-matching algorthm. In this demonstration, we performed multilayer meta-matching with 100 example subjects.\n",
    "\n",
    "Package needed (and version this jupyter notebook tested):\n",
    "* Numpy (1.19.5)\n",
    "* Scipy (1.10.1)\n",
    "* PyTorch (2.0.0)\n",
    "* Scikit-learn (0.23.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Setup\n",
    "Please modify the `path_repo` below to your repo position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_repo = '../' # '/home/the/deepGround/code/2002/Meta_matching_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialization and random seed set\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import scipy\n",
    "import torch\n",
    "import pickle\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Load data\n",
    "Load the example data that we provided, it contains \n",
    "* Example input functional connectivity (FC) `x` with size of (100, 87571)\n",
    "    * 100 is number of subjects\n",
    "    * 87571 is flatten vector of 419 by 419 FC (419*418/2=87571)\n",
    "* Example output phenotypes `y` with size of (100, 3)\n",
    "    * 3 is number of phenotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 87571) (100, 3)\n"
     ]
    }
   ],
   "source": [
    "path_v20 = os.path.join(path_repo, 'v2.0')\n",
    "model_v20_path = os.path.join(path_v20, 'models')\n",
    "sys.path.append(os.path.join(path_repo, \"utils\"))\n",
    "# check whether v2.0 model files exist and are up-to-date\n",
    "from CBIG_model_pytorch import check_models_v20\n",
    "check_models_v20(model_v20_path)\n",
    "# load data\n",
    "from CBIG_model_pytorch import demean_norm\n",
    "path_data = os.path.join(path_repo, 'data')\n",
    "npz = np.load(os.path.join(path_data, 'meta_matching_example_data.npz'))\n",
    "x_input = npz['x']\n",
    "y_input = npz['y']\n",
    "x_input = demean_norm(x_input)\n",
    "\n",
    "print(x_input.shape, y_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Split data\n",
    "Here, we also split 100 subjects to 80/20, where 80 for training, and 20 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 87571) (20, 87571) (80, 3) (20, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_input, y_input, test_size=0.2, random_state=42)\n",
    "n_subj_train, n_subj_test = x_train.shape[0], x_test.shape[0]\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data split, we z-normalize the label (y) of the training and testing set, using mean and std of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_norm(y_train, y_test):\n",
    "    # subtract mean of y of the training set\n",
    "    t_mu = np.nanmean(y_train, axis=0, keepdims=True)\n",
    "    y_train = y_train - t_mu\n",
    "    y_test = y_test - t_mu\n",
    "\n",
    "    # divide std of y of the training set\n",
    "    t_sigma = np.nanstd(y_train, axis=0)\n",
    "    if y_train.ndim == 2:\n",
    "        t_sigma_d = t_sigma[np.newaxis, :]\n",
    "    else:\n",
    "        t_sigma_d = t_sigma\n",
    "        if t_sigma == 0:\n",
    "            return y_train, y_test\n",
    "    y_train = y_train / t_sigma_d\n",
    "    y_test = y_test / t_sigma_d\n",
    "    return y_train, y_test\n",
    "\n",
    "y_train, y_test = z_norm(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Multilayer meta-matching models predict\n",
    "Here we apply the DNN and RR models trained on extra-large source dataset (UK Biobank), large source dataset (ABCD) and medium source dataset (GSP, HBN and eNKI) to predict source phenotypes on `x_train` and `x_test`. We will get the predicted 458 source phenotypes on both 80 training subjects and 20 test subjects (`y_train_pred` and `y_test_pred`), and the corresponding phenotype names (`y_names`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458\n",
      "(80, 458) \n",
      " [[-0.10125838  0.03613428 -0.46239799 ...  0.39725083 -0.76892095\n",
      "  -1.44623037]\n",
      " [ 0.10885195 -0.16720149  0.49574935 ...  3.06916403  2.31081115\n",
      "   0.82206434]\n",
      " [ 0.10951973  0.2454676  -0.44824579 ... -0.80978381 -0.17417671\n",
      "  -0.14402823]\n",
      " ...\n",
      " [ 0.13027874  0.01848663 -0.1462169  ...  0.85882294  0.5268896\n",
      "   0.34999364]\n",
      " [-0.00888915 -0.21047002  0.36738572 ...  1.74289951  0.60727872\n",
      "   0.73831846]\n",
      " [-0.08907671  0.17226079 -0.29705456 ... -0.22615359  0.2034359\n",
      "   0.777701  ]]\n",
      "(20, 458) \n",
      " [[-0.06638002 -0.1506063   0.14014049 ...  1.52472606  1.45798438\n",
      "   0.87642115]\n",
      " [-0.01658378 -0.20561379  0.38617522 ...  1.10521609 -0.05357165\n",
      "   1.36535158]\n",
      " [ 0.04155716 -0.19629928  0.32053086 ...  1.45453133  0.99876183\n",
      "   1.01717683]\n",
      " ...\n",
      " [ 0.05204854 -0.173134    0.32241014 ...  1.38678291  1.38245556\n",
      "  -0.23924551]\n",
      " [-0.17038341 -0.13663808  0.15161921 ...  1.81635199  0.3067782\n",
      "   1.71055638]\n",
      " [ 0.10234474  0.19847694 -0.43693691 ...  0.65697633  1.39920397\n",
      "   0.85434645]]\n"
     ]
    }
   ],
   "source": [
    "# Inference using all DNN + RR models from all source datasets\n",
    "from CBIG_model_pytorch import multilayer_metamatching_infer\n",
    "dataset_names = {'extra-large': 'UKBB', 'large': 'ABCD', 'medium': ['GSP', 'HBN', 'eNKI']}\n",
    "y_train_pred, y_names = multilayer_metamatching_infer(x_train, y_train, model_v20_path, dataset_names)\n",
    "y_test_pred, _ = multilayer_metamatching_infer(x_test, y_test, model_v20_path, dataset_names)\n",
    "\n",
    "print(len(y_names))\n",
    "print(y_train_pred.shape, '\\n', y_train_pred)\n",
    "print(y_test_pred.shape, '\\n', y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Stacking\n",
    "Perform stacking with `y_train_pred`, `y_test_pred`, `y_train`, where we use the prediction of 80 subjects `y_train_pred` (input) and real data `y_train` (output) to train the stacking model (you can either use all 67 source phenotypes for stacking, or select top K source phenotypes relevant to the target phenotype, like we mentioned in our paper; it turns out that these 2 ways achieves similar performances), then we applied the model to `y_test_pred` to get final prediction of 3 phenotypes on 20 subjects.\n",
    "\n",
    "#### Hyperparameter Tuning \n",
    "In `stacking()` function, we set the range of `alpha` as `[0.00001, 0.0001, 0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1, 1.5, 2, 2.5, 3, 3.5, 4, 5, 10, 15, 20]`. You are weclomed to modify the range of `alpha` to get better performance on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 3) \n",
      " [[ 0.31547324  0.429776    0.24438339]\n",
      " [ 0.48541949  0.39782572  0.29578741]\n",
      " [ 0.2405772   0.3119502   0.49131335]\n",
      " [-0.54862517 -0.52383948 -0.4586865 ]\n",
      " [ 0.23397184  0.09786208 -0.12051541]\n",
      " [-0.00592956 -0.05972484 -0.58517721]\n",
      " [ 0.22268349  0.28420622  0.27605983]\n",
      " [-0.37521873 -0.34830537 -0.48129185]\n",
      " [-0.42836626 -0.48787542 -0.00697907]\n",
      " [ 0.17523012  0.03362288  0.13360624]\n",
      " [-0.36981469 -0.45021176 -0.36496257]\n",
      " [ 0.12878666  0.22143727  0.01007384]\n",
      " [ 0.32556518  0.29869739  0.20836058]\n",
      " [ 0.31937476  0.3363618   0.35522556]\n",
      " [-0.00916185 -0.05389288  0.22052705]\n",
      " [ 0.29450547  0.47023931  0.29891575]\n",
      " [-0.4655142  -0.27739325  0.13200606]\n",
      " [ 0.04682419  0.05586322 -0.64544138]\n",
      " [ 0.18472115  0.20659532  0.77945917]\n",
      " [-0.14501365 -0.17101814 -0.06838476]]\n"
     ]
    }
   ],
   "source": [
    "from CBIG_model_pytorch import stacking\n",
    "y_test_final=np.zeros((y_test_pred.shape[0], y_train.shape[1]))\n",
    "for i in range(y_train.shape[1]):\n",
    "    # For each test phenotype, perform stacking by developing a KRR model\n",
    "    y_test_temp, _ = stacking(y_train_pred, y_test_pred, y_train[:,i].view(), [0.00001, 0.0001, 0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1, 1.5, 2, 2.5, 3, 3.5, 4, 5, 10, 15, 20])\n",
    "    y_test_final[:,i] = y_test_temp.flatten()\n",
    "print(y_test_final.shape, '\\n', y_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Evaluation\n",
    "Evaluate the prediction performance. Note that we didn't reverse the z-normalization which previously applied on `y_train` and `y_test`. This is because the metric (Pearson correlation) would not be affected by z-normalization. If you are predicting the phenotypes for practical use, you are recommended to reverse the z-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39993445 0.33584784 0.36800187]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "corr = np.zeros((y_train.shape[1]))\n",
    "for i in range(y_train.shape[1]):\n",
    "    corr[i] = pearsonr(y_test_final[:, i], y_test[:, i])[0]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Haufe transform predictive network features (PNFs) computation\n",
    "Here we compute the PNF for stacking we just performed. It computes the covariance between 3 phenotype prediciton and each element of FC on the 80 training subjects, which helps identify how changes in FC are associated with increases or decreases in the phenotypic prediction. This process enables generating feature importance weights for each edge, indicating the contribution of each feature to the prediction. The final PNF is in shape of (87571, 3), where 87571 is number of 419 by 419 FC elements, and 3 is number of phenotypes. You can visualize PNF matrix in specific network order by calling our MATLAB function [CBIG_PlotCorrMatNetOrder](https://github.com/ThomasYeoLab/CBIG/blob/master/utilities/matlab/figure_utilities/PlotCorrMat/CBIG_PlotCorrMatNetOrder.m) (set res=400, netorder='Schaefer_Yeo17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 3)\n",
      "[[-1.54667507e-04 -1.32399773e-04 -1.31030976e-04]\n",
      " [-4.07118465e-05 -3.62762901e-05 -1.60762181e-04]\n",
      " [-1.74738927e-04 -1.50471013e-04 -3.19341539e-04]\n",
      " ...\n",
      " [ 1.21284993e-04  8.06889920e-05  1.50644705e-04]\n",
      " [ 1.65970726e-04  1.25041515e-04  1.95054270e-04]\n",
      " [ 1.15016185e-04  1.10306331e-04 -3.75667195e-05]] \n",
      " (87571, 3)\n"
     ]
    }
   ],
   "source": [
    "from CBIG_model_pytorch import covariance_rowwise\n",
    "\n",
    "y_train_haufe, _ = stacking(y_train_pred, y_train_pred, y_train)\n",
    "print(y_train_haufe.shape)\n",
    "cov = covariance_rowwise(x_train, y_train_haufe)\n",
    "print(cov, '\\n', cov.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Haufe transform predictive network features (PNFs) computation for training phenotypes\n",
    "Here we compute the PNF for stacking we just performed. It computes the covariance between 3 phenotype prediciton and each training phenotypes on the 80 training subjects, which helps identify which source phenotypes are more associated with target phenotype prediction. The final PNF is in shape of (458, 3), where 458 is the number of source phenotypic predictions, and 3 is number of phenotypes.\n",
    "\n",
    "We also give the phenotype name of output in order, which is in format of \\<Phenotype_name\\>\\_\\<Dataset\\>\\_\\<Model\\>. The \\<Model\\> can be DNN/KRR base model trained on UK Biobank ('DNN'/'KRR'), one layer meta-matching ('1layer'), or two layer meta-matching ('2layer')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0054045   0.00075897 -0.01404931]\n",
      " [-0.0411503  -0.0377332   0.00257934]\n",
      " [ 0.10179323  0.09308122  0.02029504]\n",
      " ...\n",
      " [ 0.17937701  0.17730432  0.02461779]\n",
      " [ 0.09936957  0.10102418  0.02913543]\n",
      " [ 0.1538624   0.1626103   0.24924063]] \n",
      " (458, 3)\n",
      "['Alcohol_1_UKBB_DNN', 'Alcohol_2_UKBB_DNN', 'Alcohol_3_UKBB_DNN', 'Time_walk_UKBB_DNN', 'Time_drive_UKBB_DNN', 'Time_TV_UKBB_DNN', 'Sleep_UKBB_DNN', 'Age_edu_UKBB_DNN', 'Work_UKBB_DNN', 'Travel_UKBB_DNN', '#household_UKBB_DNN', 'Neuro_UKBB_DNN', 'Hearing_UKBB_DNN', 'Fluid_Int._UKBB_DNN', 'Match_UKBB_DNN', 'Sex_UKBB_DNN', 'Match-o_UKBB_DNN', 'Age_UKBB_DNN', 'Trail-o_C1_UKBB_DNN', 'Trail-o_C4_UKBB_DNN', 'Digit-o_C1_UKBB_DNN', 'Digit-o_C6_UKBB_DNN', 'Sex_G_C1_UKBB_DNN', 'Sex_G_C2_UKBB_DNN', 'Genetic_C1_UKBB_DNN', 'Cancer_C1_UKBB_DNN', 'Urine_C1_UKBB_DNN', 'Blood_C2_UKBB_DNN', 'Blood_C3_UKBB_DNN', 'Blood_C4_UKBB_DNN', 'Blood_C5_UKBB_DNN', 'Deprive_C1_UKBB_DNN', 'Dur_C1_UKBB_DNN', 'Dur_C2_UKBB_DNN', 'Dur_C4_UKBB_DNN', 'Trail_C1_UKBB_DNN', 'Tower_C1_UKBB_DNN', 'Digit_1_UKBB_DNN', 'RT_C1_UKBB_DNN', 'ProMem_C1_UKBB_DNN', '#Mem_C1_UKBB_DNN', 'Matrix_C1_UKBB_DNN', 'Matrix_C2_UKBB_DNN', 'Matrix_C3_UKBB_DNN', 'Illness_C1_UKBB_DNN', 'Illness_C4_UKBB_DNN', 'Loc_C1_UKBB_DNN', 'Breath_C1_UKBB_DNN', 'Grip_C1_UKBB_DNN', 'ECG_C1_UKBB_DNN', 'ECG_C2_UKBB_DNN', 'ECG_C3_UKBB_DNN', 'ECG_C6_UKBB_DNN', 'Carotid_C1_UKBB_DNN', 'Carotid_C5_UKBB_DNN', 'Bone_C1_UKBB_DNN', 'Bone_C3_UKBB_DNN', 'Body_C1_UKBB_DNN', 'Body_C2_UKBB_DNN', 'Body_C3_UKBB_DNN', 'BP_eye_C2_UKBB_DNN', 'BP_eye_C3_UKBB_DNN', 'BP_eye_C4_UKBB_DNN', 'BP_eye_C5_UKBB_DNN', 'BP_eye_C6_UKBB_DNN', 'Family_C1_UKBB_DNN', 'Smoke_C1_UKBB_DNN', 'Alcohol_1_UKBB_KRR', 'Alcohol_2_UKBB_KRR', 'Alcohol_3_UKBB_KRR', 'Time_walk_UKBB_KRR', 'Time_drive_UKBB_KRR', 'Time_TV_UKBB_KRR', 'Sleep_UKBB_KRR', 'Age_edu_UKBB_KRR', 'Work_UKBB_KRR', 'Travel_UKBB_KRR', '#household_UKBB_KRR', 'Neuro_UKBB_KRR', 'Hearing_UKBB_KRR', 'Fluid_Int._UKBB_KRR', 'Match_UKBB_KRR', 'Sex_UKBB_KRR', 'Match-o_UKBB_KRR', 'Age_UKBB_KRR', 'Trail-o_C1_UKBB_KRR', 'Trail-o_C4_UKBB_KRR', 'Digit-o_C1_UKBB_KRR', 'Digit-o_C6_UKBB_KRR', 'Sex_G_C1_UKBB_KRR', 'Sex_G_C2_UKBB_KRR', 'Genetic_C1_UKBB_KRR', 'Cancer_C1_UKBB_KRR', 'Urine_C1_UKBB_KRR', 'Blood_C2_UKBB_KRR', 'Blood_C3_UKBB_KRR', 'Blood_C4_UKBB_KRR', 'Blood_C5_UKBB_KRR', 'Deprive_C1_UKBB_KRR', 'Dur_C1_UKBB_KRR', 'Dur_C2_UKBB_KRR', 'Dur_C4_UKBB_KRR', 'Trail_C1_UKBB_KRR', 'Tower_C1_UKBB_KRR', 'Digit_1_UKBB_KRR', 'RT_C1_UKBB_KRR', 'ProMem_C1_UKBB_KRR', '#Mem_C1_UKBB_KRR', 'Matrix_C1_UKBB_KRR', 'Matrix_C2_UKBB_KRR', 'Matrix_C3_UKBB_KRR', 'Illness_C1_UKBB_KRR', 'Illness_C4_UKBB_KRR', 'Loc_C1_UKBB_KRR', 'Breath_C1_UKBB_KRR', 'Grip_C1_UKBB_KRR', 'ECG_C1_UKBB_KRR', 'ECG_C2_UKBB_KRR', 'ECG_C3_UKBB_KRR', 'ECG_C6_UKBB_KRR', 'Carotid_C1_UKBB_KRR', 'Carotid_C5_UKBB_KRR', 'Bone_C1_UKBB_KRR', 'Bone_C3_UKBB_KRR', 'Body_C1_UKBB_KRR', 'Body_C2_UKBB_KRR', 'Body_C3_UKBB_KRR', 'BP_eye_C2_UKBB_KRR', 'BP_eye_C3_UKBB_KRR', 'BP_eye_C4_UKBB_KRR', 'BP_eye_C5_UKBB_KRR', 'BP_eye_C6_UKBB_KRR', 'Family_C1_UKBB_KRR', 'Smoke_C1_UKBB_KRR', 'cbcl_scr_syn_anxdep_r_ABCD_1layer', 'cbcl_scr_syn_withdep_r_ABCD_1layer', 'cbcl_scr_syn_somatic_r_ABCD_1layer', 'cbcl_scr_syn_social_r_ABCD_1layer', 'cbcl_scr_syn_thought_r_ABCD_1layer', 'cbcl_scr_syn_attention_r_ABCD_1layer', 'cbcl_scr_syn_rulebreak_r_ABCD_1layer', 'cbcl_scr_syn_aggressive_r_ABCD_1layer', 'nihtbx_picvocab_uncorrected_ABCD_1layer', 'nihtbx_flanker_uncorrected_ABCD_1layer', 'nihtbx_list_uncorrected_ABCD_1layer', 'nihtbx_cardsort_uncorrected_ABCD_1layer', 'nihtbx_pattern_uncorrected_ABCD_1layer', 'nihtbx_picture_uncorrected_ABCD_1layer', 'nihtbx_reading_uncorrected_ABCD_1layer', 'nihtbx_fluidcomp_uncorrected_ABCD_1layer', 'nihtbx_cryst_uncorrected_ABCD_1layer', 'nihtbx_totalcomp_uncorrected_ABCD_1layer', 'upps_y_ss_negative_urgency_ABCD_1layer', 'upps_y_ss_lack_of_planning_ABCD_1layer', 'upps_y_ss_sensation_seeking_ABCD_1layer', 'upps_y_ss_positive_urgency_ABCD_1layer', 'upps_y_ss_lack_of_perseverance_ABCD_1layer', 'bis_y_ss_bis_sum_ABCD_1layer', 'bis_y_ss_bas_rr_ABCD_1layer', 'bis_y_ss_bas_drive_ABCD_1layer', 'bis_y_ss_bas_fs_ABCD_1layer', 'pps_y_ss_number_ABCD_1layer', 'pps_y_ss_severity_score_ABCD_1layer', 'pgbi_p_ss_score_ABCD_1layer', 'pea_ravlt_sd_trial_vi_tc_ABCD_1layer', 'pea_ravlt_ld_trial_vii_tc_ABCD_1layer', 'pea_wiscv_trs_ABCD_1layer', 'lmt_scr_perc_correct_ABCD_1layer', 'lmt_scr_rt_correct_ABCD_1layer', 'lmt_scr_efficiency_ABCD_1layer', 'Flank_S_CORRpc_GSP_1layer', 'avg_MenRot_non0_CORRpc_GSP_1layer', 'Shipley_Vocab_Raw_GSP_1layer', 'Matrix_WAIS_GSP_1layer', 'STAI_tAnxiety_GSP_1layer', 'STAI_sAnxiety_GSP_1layer', 'POMS_TotMdDisturb_GSP_1layer', 'Barratt_tot_GSP_1layer', 'MindWandering_Freq_GSP_1layer', 'NEO_N_GSP_1layer', 'NEO_E_GSP_1layer', 'NEO_O_GSP_1layer', 'NEO_A_GSP_1layer', 'NEO_C_GSP_1layer', 'TCI_Novelty_GSP_1layer', 'TCI_RewardDependence_GSP_1layer', 'TCI_HarmAvoidance_GSP_1layer', 'DOSPERT_taking_GSP_1layer', 'DOSPERT_perception_GSP_1layer', 'BISBAS_BAS_Drive_GSP_1layer', 'BISBAS_BAS_Fun_GSP_1layer', 'BISBAS_BAS_Reward_GSP_1layer', 'BISBAS_BIS_GSP_1layer', 'SDQ_Conduct_Problems_HBN_1layer', 'SDQ_Difficulties_Total_HBN_1layer', 'SDQ_Emotional_Problems_HBN_1layer', 'SDQ_Externalizing_HBN_1layer', 'SDQ_Generating_Impact_HBN_1layer', 'SDQ_Hyperactivity_HBN_1layer', 'SDQ_Internalizing_HBN_1layer', 'SDQ_Peer_Problems_HBN_1layer', 'SDQ_Prosocial_HBN_1layer', 'SRS_AWR_T_HBN_1layer', 'SRS_COG_T_HBN_1layer', 'SRS_COM_T_HBN_1layer', 'SRS_DSMRRB_T_HBN_1layer', 'SRS_MOT_T_HBN_1layer', 'SRS_RRB_T_HBN_1layer', 'SRS_SCI_T_HBN_1layer', 'SCQ_Total_HBN_1layer', 'ASSQ_Total_HBN_1layer', 'SWAN_IN_Avg_HBN_1layer', 'SWAN_HY_Avg_HBN_1layer', 'ARI_S_Total_Score_HBN_1layer', 'ARI_P_Total_Score_HBN_1layer', 'CBCL_AD_T_HBN_1layer', 'CBCL_WD_T_HBN_1layer', 'CBCL_SC_T_HBN_1layer', 'CBCL_SP_T_HBN_1layer', 'CBCL_TP_T_HBN_1layer', 'CBCL_AP_T_HBN_1layer', 'CBCL_RBB_T_HBN_1layer', 'CBCL_AB_T_HBN_1layer', 'CBCL_OP_HBN_1layer', 'CBCL_Int_T_HBN_1layer', 'CBCL_Ext_T_HBN_1layer', 'CELF_Total_HBN_1layer', 'WIAT_Num_Stnd_HBN_1layer', 'WIAT_Pseudo_Stnd_HBN_1layer', 'WIAT_Spell_Stnd_HBN_1layer', 'WIAT_Word_Stnd_HBN_1layer', 'NIH7_Card_HBN_1layer', 'NIH7_Flanker_HBN_1layer', 'NIH7_List_HBN_1layer', 'NIH7_Pattern_HBN_1layer', 'ANT_01_eNKI_1layer', 'ANT_02_eNKI_1layer', 'ANT_03_eNKI_1layer', 'DF_23_eNKI_1layer', 'DF_25_eNKI_1layer', 'DF_27_eNKI_1layer', 'DF_29_eNKI_1layer', 'DKEFSTMT_18_eNKI_1layer', 'DKEFSTMT_19_eNKI_1layer', 'DKEFSTMT_20_eNKI_1layer', 'DKEFSTMT_21_eNKI_1layer', 'DKEFSTMT_22_eNKI_1layer', 'DKEFSTMT_24_eNKI_1layer', 'DKEFSTMT_26_eNKI_1layer', 'DKEFSTMT_28_eNKI_1layer', 'DKEFSTMT_30_eNKI_1layer', 'DKEFSTMT_32_eNKI_1layer', 'DKEFSTMT_34_eNKI_1layer', 'TOWER_47_eNKI_1layer', 'TOWER_49_eNKI_1layer', 'TOWER_51_eNKI_1layer', 'TOWER_53_eNKI_1layer', 'TOWER_55_eNKI_1layer', 'TOWER_57_eNKI_1layer', 'DEHQ_16_eNKI_1layer', 'PTSDCH_55_eNKI_1layer', 'INT_12_eNKI_1layer', 'INT_13_eNKI_1layer', 'INT_14_eNKI_1layer', 'WIAT_04_eNKI_1layer', 'WIAT_05_eNKI_1layer', 'WIAT_06_eNKI_1layer', 'WIAT_08_eNKI_1layer', 'PENNCNP_0136_eNKI_1layer', 'PENNCNP_0137_eNKI_1layer', 'PENNCNP_0139_eNKI_1layer', 'PENNCNP_0140_eNKI_1layer', 'PENNCNP_0131_eNKI_1layer', 'VF_32_eNKI_1layer', 'VF_34_eNKI_1layer', 'VF_35_eNKI_1layer', 'VF_37_eNKI_1layer', 'VF_39_eNKI_1layer', 'VF_41_eNKI_1layer', 'VF_43_eNKI_1layer', 'VF_45_eNKI_1layer', 'VF_47_eNKI_1layer', 'VF_49_eNKI_1layer', 'VF_51_eNKI_1layer', 'VF_54_eNKI_1layer', 'DKEFSCWI_13_eNKI_1layer', 'DKEFSCWI_14_eNKI_1layer', 'DKEFSCWI_15_eNKI_1layer', 'DKEFSCWI_16_eNKI_1layer', 'DKEFSCWI_18_eNKI_1layer', 'DKEFSCWI_20_eNKI_1layer', 'DKEFSCWI_22_eNKI_1layer', 'DKEFSCWI_24_eNKI_1layer', 'DKEFSCWI_26_eNKI_1layer', 'DKEFSCWI_28_eNKI_1layer', 'DKEFSCWI_40_eNKI_1layer', 'cbcl_scr_syn_anxdep_r_ABCD_2layer', 'cbcl_scr_syn_withdep_r_ABCD_2layer', 'cbcl_scr_syn_somatic_r_ABCD_2layer', 'cbcl_scr_syn_social_r_ABCD_2layer', 'cbcl_scr_syn_thought_r_ABCD_2layer', 'cbcl_scr_syn_attention_r_ABCD_2layer', 'cbcl_scr_syn_rulebreak_r_ABCD_2layer', 'cbcl_scr_syn_aggressive_r_ABCD_2layer', 'nihtbx_picvocab_uncorrected_ABCD_2layer', 'nihtbx_flanker_uncorrected_ABCD_2layer', 'nihtbx_list_uncorrected_ABCD_2layer', 'nihtbx_cardsort_uncorrected_ABCD_2layer', 'nihtbx_pattern_uncorrected_ABCD_2layer', 'nihtbx_picture_uncorrected_ABCD_2layer', 'nihtbx_reading_uncorrected_ABCD_2layer', 'nihtbx_fluidcomp_uncorrected_ABCD_2layer', 'nihtbx_cryst_uncorrected_ABCD_2layer', 'nihtbx_totalcomp_uncorrected_ABCD_2layer', 'upps_y_ss_negative_urgency_ABCD_2layer', 'upps_y_ss_lack_of_planning_ABCD_2layer', 'upps_y_ss_sensation_seeking_ABCD_2layer', 'upps_y_ss_positive_urgency_ABCD_2layer', 'upps_y_ss_lack_of_perseverance_ABCD_2layer', 'bis_y_ss_bis_sum_ABCD_2layer', 'bis_y_ss_bas_rr_ABCD_2layer', 'bis_y_ss_bas_drive_ABCD_2layer', 'bis_y_ss_bas_fs_ABCD_2layer', 'pps_y_ss_number_ABCD_2layer', 'pps_y_ss_severity_score_ABCD_2layer', 'pgbi_p_ss_score_ABCD_2layer', 'pea_ravlt_sd_trial_vi_tc_ABCD_2layer', 'pea_ravlt_ld_trial_vii_tc_ABCD_2layer', 'pea_wiscv_trs_ABCD_2layer', 'lmt_scr_perc_correct_ABCD_2layer', 'lmt_scr_rt_correct_ABCD_2layer', 'lmt_scr_efficiency_ABCD_2layer', 'Flank_S_CORRpc_GSP_2layer', 'avg_MenRot_non0_CORRpc_GSP_2layer', 'Shipley_Vocab_Raw_GSP_2layer', 'Matrix_WAIS_GSP_2layer', 'STAI_tAnxiety_GSP_2layer', 'STAI_sAnxiety_GSP_2layer', 'POMS_TotMdDisturb_GSP_2layer', 'Barratt_tot_GSP_2layer', 'MindWandering_Freq_GSP_2layer', 'NEO_N_GSP_2layer', 'NEO_E_GSP_2layer', 'NEO_O_GSP_2layer', 'NEO_A_GSP_2layer', 'NEO_C_GSP_2layer', 'TCI_Novelty_GSP_2layer', 'TCI_RewardDependence_GSP_2layer', 'TCI_HarmAvoidance_GSP_2layer', 'DOSPERT_taking_GSP_2layer', 'DOSPERT_perception_GSP_2layer', 'BISBAS_BAS_Drive_GSP_2layer', 'BISBAS_BAS_Fun_GSP_2layer', 'BISBAS_BAS_Reward_GSP_2layer', 'BISBAS_BIS_GSP_2layer', 'SDQ_Conduct_Problems_HBN_2layer', 'SDQ_Difficulties_Total_HBN_2layer', 'SDQ_Emotional_Problems_HBN_2layer', 'SDQ_Externalizing_HBN_2layer', 'SDQ_Generating_Impact_HBN_2layer', 'SDQ_Hyperactivity_HBN_2layer', 'SDQ_Internalizing_HBN_2layer', 'SDQ_Peer_Problems_HBN_2layer', 'SDQ_Prosocial_HBN_2layer', 'SRS_AWR_T_HBN_2layer', 'SRS_COG_T_HBN_2layer', 'SRS_COM_T_HBN_2layer', 'SRS_DSMRRB_T_HBN_2layer', 'SRS_MOT_T_HBN_2layer', 'SRS_RRB_T_HBN_2layer', 'SRS_SCI_T_HBN_2layer', 'SCQ_Total_HBN_2layer', 'ASSQ_Total_HBN_2layer', 'SWAN_IN_Avg_HBN_2layer', 'SWAN_HY_Avg_HBN_2layer', 'ARI_S_Total_Score_HBN_2layer', 'ARI_P_Total_Score_HBN_2layer', 'CBCL_AD_T_HBN_2layer', 'CBCL_WD_T_HBN_2layer', 'CBCL_SC_T_HBN_2layer', 'CBCL_SP_T_HBN_2layer', 'CBCL_TP_T_HBN_2layer', 'CBCL_AP_T_HBN_2layer', 'CBCL_RBB_T_HBN_2layer', 'CBCL_AB_T_HBN_2layer', 'CBCL_OP_HBN_2layer', 'CBCL_Int_T_HBN_2layer', 'CBCL_Ext_T_HBN_2layer', 'CELF_Total_HBN_2layer', 'WIAT_Num_Stnd_HBN_2layer', 'WIAT_Pseudo_Stnd_HBN_2layer', 'WIAT_Spell_Stnd_HBN_2layer', 'WIAT_Word_Stnd_HBN_2layer', 'NIH7_Card_HBN_2layer', 'NIH7_Flanker_HBN_2layer', 'NIH7_List_HBN_2layer', 'NIH7_Pattern_HBN_2layer', 'ANT_01_eNKI_2layer', 'ANT_02_eNKI_2layer', 'ANT_03_eNKI_2layer', 'DF_23_eNKI_2layer', 'DF_25_eNKI_2layer', 'DF_27_eNKI_2layer', 'DF_29_eNKI_2layer', 'DKEFSTMT_18_eNKI_2layer', 'DKEFSTMT_19_eNKI_2layer', 'DKEFSTMT_20_eNKI_2layer', 'DKEFSTMT_21_eNKI_2layer', 'DKEFSTMT_22_eNKI_2layer', 'DKEFSTMT_24_eNKI_2layer', 'DKEFSTMT_26_eNKI_2layer', 'DKEFSTMT_28_eNKI_2layer', 'DKEFSTMT_30_eNKI_2layer', 'DKEFSTMT_32_eNKI_2layer', 'DKEFSTMT_34_eNKI_2layer', 'TOWER_47_eNKI_2layer', 'TOWER_49_eNKI_2layer', 'TOWER_51_eNKI_2layer', 'TOWER_53_eNKI_2layer', 'TOWER_55_eNKI_2layer', 'TOWER_57_eNKI_2layer', 'DEHQ_16_eNKI_2layer', 'PTSDCH_55_eNKI_2layer', 'INT_12_eNKI_2layer', 'INT_13_eNKI_2layer', 'INT_14_eNKI_2layer', 'WIAT_04_eNKI_2layer', 'WIAT_05_eNKI_2layer', 'WIAT_06_eNKI_2layer', 'WIAT_08_eNKI_2layer', 'PENNCNP_0136_eNKI_2layer', 'PENNCNP_0137_eNKI_2layer', 'PENNCNP_0139_eNKI_2layer', 'PENNCNP_0140_eNKI_2layer', 'PENNCNP_0131_eNKI_2layer', 'VF_32_eNKI_2layer', 'VF_34_eNKI_2layer', 'VF_35_eNKI_2layer', 'VF_37_eNKI_2layer', 'VF_39_eNKI_2layer', 'VF_41_eNKI_2layer', 'VF_43_eNKI_2layer', 'VF_45_eNKI_2layer', 'VF_47_eNKI_2layer', 'VF_49_eNKI_2layer', 'VF_51_eNKI_2layer', 'VF_54_eNKI_2layer', 'DKEFSCWI_13_eNKI_2layer', 'DKEFSCWI_14_eNKI_2layer', 'DKEFSCWI_15_eNKI_2layer', 'DKEFSCWI_16_eNKI_2layer', 'DKEFSCWI_18_eNKI_2layer', 'DKEFSCWI_20_eNKI_2layer', 'DKEFSCWI_22_eNKI_2layer', 'DKEFSCWI_24_eNKI_2layer', 'DKEFSCWI_26_eNKI_2layer', 'DKEFSCWI_28_eNKI_2layer', 'DKEFSCWI_40_eNKI_2layer']\n"
     ]
    }
   ],
   "source": [
    "cov = covariance_rowwise(y_train_pred, y_train_haufe)\n",
    "print(cov, '\\n', cov.shape)\n",
    "print(y_names) # source phenotype names in order"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
