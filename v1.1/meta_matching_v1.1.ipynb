{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta matching v1.0\n",
    "This jupyter notebook demonstrates you how to load and use meta-matching algorthm. In this demonstration, we performed Meta-matching (DNN) stacking with 100 example subjects.\n",
    "\n",
    "Package needed (and version this jupyter notebook tested):\n",
    "* Numpy (1.16.4)\n",
    "* Scipy (1.0.0)\n",
    "* PyTorch (1.3.1)\n",
    "* Scikit-learn (0.21.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Setup\n",
    "Please modify the `path_repo` below to your repo position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_repo = '../'#'/home/the/deepGround/code/2002/Meta_matching_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialization and random seed set\n",
    "\n",
    "import os\n",
    "import scipy\n",
    "import torch\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please modify the gpu number here if you want to use different gpu. If the gpu you assigned not availiable, it will assign to cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. load data\n",
    "Load the example data that we provided, it contains \n",
    "* Example input functional connectivity (FC) `x` with size of (100, 87571)\n",
    "    * 100 is number of subjects\n",
    "    * 87571 is flatten vector of 419 by 419 FC (419*418/2=87571)\n",
    "    * We perform subject-wise z-normalization, using mean and std of 87571 elements of each subject\n",
    "* Example output phenotypes `y` with size of (100, 3)\n",
    "    * 3 is number of phenotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'y']\n",
      "(100, 87571) (100, 3)\n"
     ]
    }
   ],
   "source": [
    "path_v11 = os.path.join(path_repo, 'v1.1')\n",
    "path_v10 = os.path.join(path_repo, 'v1.0')\n",
    "sys.path.append(path_v10)\n",
    "from CBIG_model_pytorch import demean_norm\n",
    "\n",
    "npz = os.path.join(path_v11, 'meta_matching_v1.1_data.npz')\n",
    "npz = np.load(npz)\n",
    "print(npz.files)\n",
    "x_input = npz['x']\n",
    "y_input = npz['y']\n",
    "x_input = demean_norm(x_input)\n",
    "print(x_input.shape, y_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Split data\n",
    "Here, we also split 100 subjects to 80/20, where 80 for training, and 20 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 87571) (20, 87571) (80, 3) (20, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_input, y_input, test_size=0.2, random_state=42)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Prepare data for PyTorch model\n",
    "Then we prepare data for DNN model, we will input both the `x_train` and `x_test` into the model to get the predicted phenotypes. \n",
    "\n",
    "For meta-matching (DNN) stacking, we do not need real phenotype for the DNN model, I created all zeros `y_dummy` just for function requirement. In some other cases, like meta-matching (DNN) finetuning, you need to use real phenotype data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from CBIG_model_pytorch import multi_task_dataset\n",
    "\n",
    "batch_size = 16\n",
    "y_dummy = np.zeros(y_train.shape)\n",
    "dset_train = multi_task_dataset(x_train, y_dummy, True)\n",
    "trainLoader = DataLoader(dset_train,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=1)\n",
    "\n",
    "y_dummy = np.zeros(y_test.shape)\n",
    "dset_test = multi_task_dataset(x_test, y_dummy, True)\n",
    "testLoader = DataLoader(dset_test,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. load model\n",
    "Here we load the meta-matching model saved, it is a DNN that takes FC as input and output 67 phenotypes prediction trained on 67 UK Biobank phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn(\n",
      "  (layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Dropout(p=0.3, inplace=False)\n",
      "      (1): Linear(in_features=87571, out_features=256, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Dropout(p=0.3, inplace=False)\n",
      "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Dropout(p=0.3, inplace=False)\n",
      "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Dropout(p=0.3, inplace=False)\n",
      "      (1): Linear(in_features=256, out_features=67, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "path_model_weight = os.path.join(path_v11, 'meta_matching_v1.1_model.pkl_torch') \n",
    "net = torch.load(path_model_weight, map_location=device)\n",
    "net.to(device)\n",
    "net.train(False)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. DNN model predict\n",
    "Here we apply the DNN trained on 67 UK Biobank phenotypes to predict the 67 phenotypes on `x_train` and `x_test`. We will get the predicted 67 phenotypes on both 80 training subjects and 20 test subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 67) \n",
      " [[-0.15115939 -0.0669269  -0.54655945 ... -0.11759652 -0.50293493\n",
      "  -0.37641898]\n",
      " [ 0.29037341 -0.20854312  0.23705281 ...  0.15359323 -0.07533579\n",
      "  -0.01452626]\n",
      " [ 0.10086952  0.20367797 -0.53936934 ... -0.04685985 -0.14448746\n",
      "  -0.0352214 ]\n",
      " ...\n",
      " [ 0.05406483 -0.03227098  0.01010649 ...  0.10382769  0.06453447\n",
      "   0.05130197]\n",
      " [-0.02054616 -0.17987496  0.16705912 ...  0.1099993  -0.22622883\n",
      "  -0.34170136]\n",
      " [-0.10911851  0.1235078  -0.43411684 ... -0.07488919 -0.53739721\n",
      "  -0.25947312]]\n",
      "(20, 67) \n",
      " [[-0.05762977 -0.17934641  0.16057274 ...  0.16511196 -0.55663419\n",
      "  -0.25849211]\n",
      " [ 0.01560813 -0.1447289   0.19979201 ...  0.13954791  0.01221326\n",
      "  -0.41223368]\n",
      " [ 0.05661141 -0.21707436  0.19259959 ...  0.1661057  -0.24444117\n",
      "  -0.16643339]\n",
      " ...\n",
      " [-0.01382111 -0.127198    0.20016842 ...  0.12121768 -0.32253277\n",
      "   0.0576634 ]\n",
      " [-0.15039177 -0.21929857  0.01748148 ...  0.12292583 -0.37921733\n",
      "  -0.48630238]\n",
      " [ 0.15801965  0.05928475 -0.45958063 ... -0.11260507  0.15486403\n",
      "  -0.21450129]]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = np.zeros((0, 67))\n",
    "for (x, _) in trainLoader:\n",
    "    x= x.to(device)\n",
    "    outputs = net(x)\n",
    "    y_train_pred = np.concatenate((y_train_pred, outputs.data.cpu().numpy()), axis=0)\n",
    "print(y_train_pred.shape, '\\n', y_train_pred)\n",
    "\n",
    "y_test_pred = np.zeros((0, 67))\n",
    "for (x, _) in testLoader:\n",
    "    x= x.to(device)\n",
    "    outputs = net(x)\n",
    "    y_test_pred = np.concatenate((y_test_pred, outputs.data.cpu().numpy()), axis=0)\n",
    "print(y_test_pred.shape, '\\n', y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Stacking\n",
    "Perform stacking with `y_train_pred`, `y_test_pred`, `y_train`, where we use the prediction of 80 subjects `y_train_pred` (input) and real data `y_train` (output) to train the stacking model (you can either use all 67 source phenotypes for stacking, or select top K source phenotypes relevant to the target phenotype, like we mentioned in our paper; it turns out that these 2 ways achieves similar performances), then we applied the model to `y_test_pred` to get final prediction of 3 phenotypes on 20 subjects.\n",
    "\n",
    "#### Hyperparameter Tuning \n",
    "In `stacking()` function, we set the range of `alpha` as `[0.00001, 0.0001, 0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1, 1.5, 2, 2.5, 3, 3.5, 4, 5, 10, 15, 20]`. You are weclomed to modify the range of `alpha` to get better performance on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 3) \n",
      " [[59.91704316 25.72586686 26.07246047]\n",
      " [66.14376526 23.98923972 24.73108215]\n",
      " [61.25782014 24.89340214 22.11663259]\n",
      " [21.40825319 13.06584165  9.37246313]\n",
      " [74.82713639 27.95233725 28.73392918]\n",
      " [49.82987774 18.36064402 19.47435966]\n",
      " [36.49521893 21.86627855 29.7535562 ]\n",
      " [34.89049406  8.73729406 13.03381664]\n",
      " [49.96686959 16.23559303 17.04143582]\n",
      " [60.55911895 23.84655006 25.25979719]\n",
      " [26.22655036 13.15274099 15.6317502 ]\n",
      " [90.25412737 39.1989077  41.48038924]\n",
      " [44.38907628 18.84119275 18.80375602]\n",
      " [69.74869482 31.5875928  34.68293162]\n",
      " [52.18927948 22.5219697  29.531368  ]\n",
      " [61.18475632 17.90580022 27.18276641]\n",
      " [39.63361043 19.58916493 20.62720972]\n",
      " [66.41021447 20.73182542 18.82197354]\n",
      " [46.6794769  20.7430434  24.27900141]\n",
      " [52.1961148  13.92751791 15.36534066]]\n"
     ]
    }
   ],
   "source": [
    "from CBIG_model_pytorch import stacking\n",
    "y_test_final=np.zeros((y_test_pred.shape[0], y_train.shape[1]))\n",
    "for i in range(y_train.shape[1]):\n",
    "    # For each test phenotype, perform stacking by developing a KRR model\n",
    "    y_test_temp, _ = stacking(y_train_pred, y_test_pred, y_train[:,i].view(), [0.00001, 0.0001, 0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1, 1.5, 2, 2.5, 3, 3.5, 4, 5, 10, 15, 20])\n",
    "    y_test_final[:,i] = y_test_temp.flatten()\n",
    "print(y_test_final.shape, '\\n', y_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Evaluation\n",
    "Evaluate the prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6493213  0.53661207 0.25953581]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "corr = np.zeros((y_train.shape[1]))\n",
    "for i in range(y_train.shape[1]):\n",
    "    corr[i] = pearsonr(y_test_final[:, i], y_test[:, i])[0]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Haufe transform predictive network features (PNFs) computation\n",
    "Here we compute the PNF for stacking we just performed. It computes the covariance between 3 phenotype prediciton and each element of FC on the 80 training subjects. The final PNF is in shape of (87571, 3), where 87571 is number of 419 by 419 FC elements, and 3 is number of phenotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 3)\n",
      "[[-9.51272534e-04 -3.33093629e-04  9.99491542e-04]\n",
      " [-4.95032426e-04 -1.34704160e-05 -2.96746334e-04]\n",
      " [-3.60040114e-03 -1.20511813e-03 -1.83170320e-03]\n",
      " ...\n",
      " [ 9.54923023e-04  1.62136461e-04  8.06833900e-04]\n",
      " [ 1.61852164e-03  7.27061452e-04  1.63064681e-03]\n",
      " [ 1.80749311e-03  8.30753940e-04  4.38313184e-04]] \n",
      " (87571, 3)\n"
     ]
    }
   ],
   "source": [
    "from CBIG_model_pytorch import covariance_rowwise\n",
    "\n",
    "y_train_haufe, _ = stacking(y_train_pred, y_train_pred, y_train)\n",
    "print(y_train_haufe.shape)\n",
    "cov = covariance_rowwise(x_train, y_train_haufe)\n",
    "print(cov, '\\n', cov.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. Haufe transform predictive network features (PNFs) computation for training phenotypes\n",
    "Here we compute the PNF for stacking we just performed. It computes the covariance between 3 phenotype prediciton and each training phenotypes on the 80 training subjects. The final PNF is in shape of (67, 3), where 67 is number of training phenotypes, and 3 is number of phenotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.14476059e-01  4.24958531e-02 -2.56837861e-02]\n",
      " [-2.35807282e-01 -1.43320430e-01 -4.63684846e-03]\n",
      " [ 8.79415505e-01  4.66306778e-01  1.75841287e-01]\n",
      " [ 1.05367594e-01  6.90872062e-02  2.35385686e-02]\n",
      " [-1.46605681e-02 -3.73209161e-02 -1.80059741e-01]\n",
      " [-1.21257278e-01 -1.50802049e-01 -3.29285698e-01]\n",
      " [ 1.24786394e-01  8.16866955e-02  1.78748205e-01]\n",
      " [ 9.03748222e-02  5.98972858e-02  2.07077366e-01]\n",
      " [ 5.15225829e-01  2.60515380e-01  1.05632279e-01]\n",
      " [-9.27633893e-02 -9.72508105e-02 -1.83468128e-01]\n",
      " [ 3.83940713e-01  2.00519881e-01  1.50639053e-01]\n",
      " [-4.30634539e-01 -2.88612709e-01 -1.17827291e-01]\n",
      " [-1.37145572e-01 -8.56516379e-02 -1.07273467e-01]\n",
      " [ 7.31631508e-01  4.57041168e-01  7.08423710e-01]\n",
      " [ 3.56652530e-01  1.78622558e-01  2.17740663e-01]\n",
      " [ 2.22915358e+00  1.29261675e+00  7.02672001e-01]\n",
      " [-5.11638474e-01 -2.58350469e-01 -2.42740146e-01]\n",
      " [-9.79363826e-02  4.49926335e-02 -1.86941683e-01]\n",
      " [-7.45365024e-01 -4.00650668e-01 -4.72234137e-01]\n",
      " [-7.88092020e-02 -4.22677762e-02 -2.99614706e-02]\n",
      " [ 5.75864724e-01  2.67335745e-01  4.41189993e-01]\n",
      " [ 2.08465636e-01  1.10379164e-01  5.03719221e-02]\n",
      " [-5.72443740e-01 -3.19970957e-01 -1.55276502e-01]\n",
      " [ 2.01902611e+00  1.17175650e+00  6.44683793e-01]\n",
      " [-2.16185732e+00 -1.13781293e+00 -8.96456137e-01]\n",
      " [ 6.23188187e-01  4.19148816e-01  7.87161103e-02]\n",
      " [-3.55717493e-02  4.16629546e-03 -7.29518635e-02]\n",
      " [-1.43847320e+00 -8.21679019e-01 -3.25235267e-01]\n",
      " [ 5.42929402e-01  3.35308395e-01  1.04330806e-01]\n",
      " [ 9.14156788e-01  4.95749309e-01  4.42153666e-01]\n",
      " [ 4.09377417e-01  2.35736156e-01  2.19793315e-01]\n",
      " [ 4.02028131e-01  2.55162838e-01  3.21871270e-01]\n",
      " [-2.62438221e-01 -1.62112864e-01 -6.75526466e-02]\n",
      " [ 1.04955027e-01  1.41753202e-01  1.85396254e-01]\n",
      " [ 6.29460122e-01  3.22987988e-01  3.11290180e-01]\n",
      " [-3.89228236e-01 -1.82725805e-01 -3.09207136e-01]\n",
      " [-8.42606712e-01 -4.44235503e-01 -5.09546711e-01]\n",
      " [ 5.87372290e-01  2.76599273e-01  4.21563547e-01]\n",
      " [-5.60327453e-01 -2.79876899e-01 -2.83752973e-01]\n",
      " [ 2.80396351e-01  1.60785192e-01  2.95405330e-01]\n",
      " [ 4.95595996e-01  2.89210302e-01  3.64788425e-01]\n",
      " [ 1.18602727e-01 -7.84119063e-04  9.20104292e-02]\n",
      " [-7.26483008e-01 -4.11217578e-01 -5.82378423e-01]\n",
      " [ 4.43526977e-01  2.53386327e-01  2.97182366e-01]\n",
      " [-5.26115285e-01 -2.62918532e-01 -4.36940602e-01]\n",
      " [-2.33448596e-01 -1.07133381e-01 -1.89199253e-01]\n",
      " [ 2.16000386e-01  1.50096717e-01  2.14182286e-02]\n",
      " [-1.92934122e+00 -1.05621339e+00 -8.26400447e-01]\n",
      " [-1.84778915e+00 -1.03992829e+00 -6.79631616e-01]\n",
      " [ 1.83477819e+00  1.05673831e+00  6.76120261e-01]\n",
      " [ 7.64845808e-03 -5.12460454e-02 -1.06620163e-01]\n",
      " [ 3.69758764e-01  1.49510847e-01  6.54390428e-02]\n",
      " [ 2.92210328e-01  2.21012262e-01 -1.02518150e-01]\n",
      " [ 2.29708699e-01  1.94816537e-01 -8.03266528e-02]\n",
      " [ 1.03022588e-02  1.58487761e-02 -5.62910406e-02]\n",
      " [ 5.38679002e-01  2.89430967e-01  7.08064072e-02]\n",
      " [ 7.70525570e-01  4.61778476e-01  2.35975105e-01]\n",
      " [-1.57625889e+00 -9.00078773e-01 -4.48715446e-01]\n",
      " [-1.57602237e+00 -9.32455555e-01 -7.06609645e-01]\n",
      " [ 3.26602732e-01  1.04063620e-01  3.69899207e-01]\n",
      " [-3.73994638e-01 -1.86953168e-01 -2.43318212e-01]\n",
      " [-2.57240673e-01 -1.61526640e-01  7.03848240e-02]\n",
      " [ 5.10660992e-01  3.51578416e-01  1.57441810e-01]\n",
      " [-1.12936724e-01 -1.04067248e-01 -1.70677224e-01]\n",
      " [ 3.88080837e-01  1.85261935e-01  5.11887244e-02]\n",
      " [-1.28617245e-01 -1.22915665e-01 -1.29513336e-01]\n",
      " [-7.66774042e-02 -6.85603270e-02 -1.32564294e-01]] \n",
      " (67, 3)\n"
     ]
    }
   ],
   "source": [
    "from CBIG_model_pytorch import covariance_rowwise\n",
    "\n",
    "cov = covariance_rowwise(y_train_pred, y_train_haufe)\n",
    "print(cov, '\\n', cov.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9211f842e4957b840c6ab516fe1756049fc8809c71ad1557ffcdfc52960e8d65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
